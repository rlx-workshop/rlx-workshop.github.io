---
layout: default
---

<div class="row">
<p>
This page contains a non-exhaustive list of resources for machine learning and reinforcement learning researchers and practitioners to learn more about offline RL. Feel free to provide additional resource suggestions via <a href="https://github.com/offline-rl-neurips/2021/edit/master/resources.html">a pull request on GitHub</a>.
</p>
</div>

<!-- <div id="tutorials" class="row">
  <h2> Survery, Tutorial and Blog Posts </h2>
  <p> <span class="smallcaps"> Levine S., Kumar A., Tucker G., Fu J.</span> 2020. Offline RL Tutorial, Review and Perspectives on Open Problems.
  <p><span class="smallcaps">Agarwal, R.</span> 2020. An optimistic perspective on offline reinforcement learning..</p>
</div> -->

<div id="references" class="row"> 
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<h2>References (Alphabetical Order)</h2>
<!-- <div id="ref-rishabh_blog">
<p><span class="smallcaps">Agarwal, R.</span> 2020. An optimistic perspective on offline reinforcement learning..</p>
</div> -->
<p>Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in Neural Information Processing Systems 35 (2022): 27730-27744.
</p>

<p>Gao, Leo, John Schulman, and Jacob Hilton. "Scaling laws for reward model overoptimization." International Conference on Machine Learning. PMLR, 2023.
</p>

<p>Leike, Jan, et al. "Scalable agent alignment via reward modeling: a research direction." arXiv preprint arXiv:1811.07871 (2018).
</p>
<p>Ziegler, Daniel M., et al. "Fine-tuning language models from human preferences." arXiv preprint arXiv:1909.08593 (2019).
</p>
<p>Rafailov, Rafael, et al. "Direct preference optimization: Your language model is secretly a reward model." arXiv preprint arXiv:2305.18290 (2023).
</p>
<p>Korbak, Tomasz, et al. "Pretraining language models with human preferences." International Conference on Machine Learning. PMLR, 2023.
</p>
<p>Song, Feifan, et al. "Preference ranking optimization for human alignment." arXiv preprint arXiv:2306.17492 (2023).
</p>
<p>Dai, Josef, et al. "Safe rlhf: Safe reinforcement learning from human feedback." arXiv preprint arXiv:2310.12773 (2023).</p>
<p>Swamy, Gokul, et al. "A minimaximalist approach to reinforcement learning from human feedback." arXiv preprint arXiv:2401.04056 (2024).
</p>
<p>Korbak, Tomasz, et al. "Pretraining language models with human preferences." International Conference on Machine Learning. PMLR, 2023.</p>
<p>Nair, Suraj, et al. "R3m: A universal visual representation for robot manipulation." arXiv preprint arXiv:2203.12601 (2022).
</p>
<p>Ma, Yecheng Jason, et al. "Vip: Towards universal visual reward and representation via value-implicit pre-training." arXiv preprint arXiv:2210.00030 (2022).
</p>
<p>Ma, Yecheng Jason, et al. "LIV: Language-Image Representations and Rewards for Robotic Control." arXiv preprint arXiv:2306.00958 (2023).</p>
<p>Ghosh, Dibya, Chethan Anand Bhateja, and Sergey Levine. "Reinforcement learning from passive data via latent intentions." International Conference on Machine Learning. PMLR, 2023</p>
<p>Park, Seohong, Oleh Rybkin, and Sergey Levine. "METRA: Scalable Unsupervised RL with Metric-Aware Abstraction." arXiv preprint arXiv:2310.08887 (2023).
</p>
<p>Zheng, Chongyi, et al. "Stabilizing Contrastive RL: Techniques for Offline Goal Reaching." arXiv preprint arXiv:2306.03346 (2023).
</p>
<p>Schmidt, Dominik, and Minqi Jiang. "Learning to Act without Actions." arXiv preprint arXiv:2312.10812 (2023).</p>
<p>Bhateja, Chethan, et al. "Robotic Offline RL from Internet Videos via Value-Function Pre-Training." arXiv preprint arXiv:2309.13041 (2023).
</p>
<p>Sun, Hao, et al. "When is Off-Policy Evaluation Useful? A Data-Centric Perspective." arXiv preprint arXiv:2311.14110 (2023).
</p>
<p>Hüyük, Alihan, Daniel Jarrett, and Mihaela van der Schaar. "Explaining by imitating: Understanding decisions by interpretable policy learning." arXiv preprint arXiv:2310.19831 (2023).
</p>
<p>Sun, Hao, et al. "Accountability in offline reinforcement learning: Explaining decisions with a corpus of examples." arXiv preprint arXiv:2310.07747 (2023).
</p>
<p>Yang, Mengjiao, et al. "Offline policy selection under uncertainty." International Conference on Artificial Intelligence and Statistics. PMLR, 2022.
</p>
<p>Eysenbach, Benjamin, Ruslan Salakhutdinov, and Sergey Levine. "C-learning: Learning to achieve goals via recursive classification." arXiv preprint arXiv:2011.08909 (2020).
</p>
<p>Zheng, Chongyi, et al. "Stabilizing Contrastive RL: Techniques for Offline Goal Reaching." arXiv preprint arXiv:2306.03346 (2023).
</p>
<p>Sikchi, Harshit, et al. "Dual rl: Unification and new methods for reinforcement and imitation learning." Sixteenth European Workshop on Reinforcement Learning. 2023.</p>
<p>Ni, T., Sikchi, H., Wang, Y., Gupta, T., Lee, L., & Eysenbach, B. (2021, October). f-irl: Inverse reinforcement learning via state marginal matching. In Conference on Robot Learning (pp. 529-551). PMLR.
</p>
<p>Sikchi, Harshit, Wenxuan Zhou, and David Held. "Learning off-policy with online planning." Conference on Robot Learning. PMLR, 2022.
</p>
<p>Sikchi, Harshit, et al. "A ranking game for imitation learning." arXiv preprint arXiv:2202.03481 (2022).
</p>
<p>Hejna, J., Rafailov, R., Sikchi, H., Finn, C., Niekum, S., Knox, W. B., & Sadigh, D. (2023). Contrastive prefence learning: Learning from human feedback without rl. arXiv preprint arXiv:2310.13639.
</p>
<p>Sikchi, Harshit, et al. "Score Models for Offline Goal-Conditioned Reinforcement Learning." arXiv preprint arXiv:2311.02013 (2023).</p>
<p>Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training</p>
<p>Ball, P. J., Smith, L., Kostrikov, I., & Levine, S. (2023). Efficient online reinforcement learning with offline data. arXiv preprint arXiv:2302.02948.
</p>
<p>Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R., ... & Martín-Martín, R. (2021). What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298.
</p>
<p>Christiano, Paul F., et al. "Deep reinforcement learning from human preferences." Advances in neural information processing systems 30 (2017).
</p>

<p>Swamy, G., Choudhury, S., Bagnell, J. A., & Wu, Z. S. (2023). Inverse Reinforcement Learning without Reinforcement Learning. arXiv e-prints, arXiv-2303.
</p>
<p>Zhu, Banghua, Jiantao Jiao, and Michael I. Jordan. "Principled Reinforcement Learning with Human Feedback from Pairwise or $ K $-wise Comparisons." arXiv preprint arXiv:2301.11270 (2023).</p>
<p>Swamy, G., Choudhury, S., Bagnell, J. A., & Wu, S. (2021, July). Of moments and matching: A game-theoretic framework for closing the imitation gap. In International Conference on Machine Learning (pp. 10022-10032). PMLR.
</p>
<p>Liu, Z., Guo, Z., Lin, H., Yao, Y., Zhu, J., Cen, Z., ... & Zhao, D. (2023). Datasets and Benchmarks for Offline Safe Reinforcement Learning. arXiv preprint arXiv:2306.09303.
</p>
<p>Cen, Z., Liu, Z., Wang, Z., Yao, Y., Lam, H., & Zhao, D. (2024). Learning from Sparse Offline Datasets via Conservative Density Estimation. International Conference on Learning Representations.
</p>
<p>Liu, Z., Guo, Z., Yao, Y., Cen, Z., Yu, W., Zhang, T., & Zhao, D. (2023). Constrained Decision Transformer for Offline Safe Reinforcement Learning. International Conference on Machine Learning. PMLR.
</p>
<p>Xu, H., Zhan, X., & Zhu, X. (2022, June). Constraints penalized q-learning for safe offline reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 36, No. 8, pp. 8753-8760).
</p>
<p>Xu, H., Zhan, X., Yin, H., & Qin, H. (2022, June). Discriminator-weighted offline imitation learning from suboptimal demonstrations. In International Conference on Machine Learning (pp. 24725-24742). PMLR.
</p>
<p>Xu, H., Jiang, L., Jianxiong, L., & Zhan, X. (2022). A policy-guided imitation approach for offline reinforcement learning. Advances in Neural Information Processing Systems, 35, 4085-4098.
</p>
<p>Mediratta, I., You, Q., Jiang, M., & Raileanu, R. (2023). The Generalization Gap in Offline Reinforcement Learning. arXiv preprint arXiv:2312.05742.
</p>
<p>Xu, H., Jiang, L., Li, J., Yang, Z., Wang, Z., Chan, V. W. K., & Zhan, X. (2023). Offline rl with no ood actions: In-sample learning via implicit value regularization. International Conference on Learning Representations</p>
<p>Kumar, A., Agarwal, R., Geng, X., Tucker, G., & Levine, S. (2022). Offline q-learning on diverse multi-task data both scales and generalizes. arXiv preprint arXiv:2211.15144.
</p>
<p>Song, Y., Zhou, Y., Sekhari, A., Bagnell, J. A., Krishnamurthy, A., & Sun, W. (2022). Hybrid rl: Using both offline and online data can make rl efficient. International Conference on Learning Representations.
</p>
<p>Li, J., Hu, X., Xu, H., Liu, J., Zhan, X., & Zhang, Y. Q. (2023). PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning. arXiv preprint arXiv:2305.15669.
</p>
<p>Li, Q., Zhang, J., Ghosh, D., Zhang, A., & Levine, S. (2023). Accelerating exploration with unlabeled prior data. Advances in Neural Information Processing Systems.
</p>

<!-- 
<div id="ref-agarwal2019striving">
<p><span class="smallcaps">Ouyang, Long, et al. </span> 2020. An Optimistic Perspective on Offline Reinforcement Learning. <em>International Conference on Machine Learning (ICML)</em>.</p>
</div>
<div id="ref-bodnar2019quantile">
<p><span class="smallcaps">Bodnar, C., Li, A., Hausman, K., Pastor, P., and Kalakrishnan, M.</span> 2019. Quantile qt-opt for risk-aware vision-based robotic grasping. <em>arXiv preprint arXiv:1910.02787</em>.</p>
</div>
<div id="ref-bottou2013counterfactual">
<p><span class="smallcaps">Bottou, L., Peters, J., Quiñonero-Candela, J., et al.</span> 2013. Counterfactual reasoning and learning systems: The example of computational advertising. <em>JMLR</em>.</p>
</div>
<div id="ref-boyan199lstdq">
<p><span class="smallcaps">Boyan, J. A.</span> 1999. Least-squares temporal difference learning <em>ICML</em>.</p>
</div>
<div id="ref-cabi2019framework">
<p><span class="smallcaps">Cabi, S., Colmenarejo, S.G., Novikov, A., et al.</span> 2019. A framework for data-driven robotics. <em>arXiv preprint arXiv:1909.12200</em>.</p>
</div>
<div id="ref-chang2020semantic">
<p><span class="smallcaps">Chang, M., Gupta, A., & Gupta, S.</span> 2020. Semantic visual navigation by watching youtube videos. <em>NeurIPS</em>.</p>
</div>
<div id="ref-chen2019information">
<p><span class="smallcaps">Chen, J. and Jiang, N.</span> 2019. Information-theoretic considerations in batch reinforcement learning. <em>ICML</em>.</p>
</div>
<div id="ref-chen2019bail">
<p><span class="smallcaps">Chen, X., Zhou, Z., Wang, Z., et al.</span> 2019. BAIL: Best-action imitation learning for batch deep reinforcement learning. <em>arXiv preprint arXiv:1910.12179</em>.</p>
</div>
<div id="ref-dai2017boosting">
<p><span class="smallcaps">Dai, B., Shaw, A., He, N., Li, L., and Song, L.</span> 2017. Boosting the actor with dual critic. <em>arXiv preprint arXiv:1712.10282</em>.</p>
</div>
<div id="ref-dai2018sbeed">
<p><span class="smallcaps">Dai, B., Shaw, A., Li, L., et al.</span> 2018. SBEED: Convergent reinforcement learning with nonlinear function approximation. <em>International conference on machine learning</em>, 1125–1134.</p>
</div>
<div id="ref-degris2012offpac">
  <p><span class="smallcaps">Degris, T., White, M., Sutton, R. S.</span> 2012. Off-policy Actor-Critic. arXiv preprint arXiv:1205.4839. </p>
<div id="ref-imagenet_cvpr09">
<p><span class="smallcaps">Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.</span> 2009. ImageNet: A Large-Scale Hierarchical Image Database. <em>CVPR</em>.</p>
</div>
<div id="ref-dudik2014doubly">
<p><span class="smallcaps">Dudı́k, M., Erhan, D., Langford, J., Li, L., and others</span>. 2014. Doubly robust policy evaluation and optimization. <em>Statistical Science</em> <em>29</em>, 4, 485–511.</p>
</div>
<div id="ref-dulac2019challenges">
<p><span class="smallcaps">Dulac-Arnold, G., Mankowitz, D., and Hester, T.</span> 2019. Challenges of real-world reinforcement learning. <em>arXiv preprint arXiv:1904.12901</em>.</p>
</div>
<div id="ref-ernst2005tree">
<p><span class="smallcaps">Ernst, D., Geurts, P., and Wehenkel, L.</span> 2005. Tree-based batch mode reinforcement learning. <em>JMLR</em>.</p>
</div>
<div id="ref-d4rl">
<p><span class="smallcaps">Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.</span> 2020. D4RL: Datasets for deep data-driven reinforcement learning. <em>ArXiv</em>.</p>
</div>
<div id="ref-farahmand2010error">
<p><span class="smallcaps">Farahmand, A. M., Szepesvári, C., Munos R.</span> 2010. Error propagation for approximate policy and value iteration  <em>NeurIPS</em>.</p>
</div>
<div id="ref-farahmand2011model">
<p><span class="smallcaps">Farahmand, A. M., Szepesvári, C.</span> 2011. Model selection in reinforcement learning. <em>Machine learning,</em> 85(3) 299-332.</p>
</div>
<div id="ref-fujimoto2019benchmarking">
<p><span class="smallcaps">Fujimoto, S., Conti, E., Ghavamzadeh, M., and Pineau, J.</span> 2019. Benchmarking batch deep reinforcement learning algorithms. <em>arXiv preprint arXiv:1910.01708</em>.</p>
</div>
<div id="ref-fujimoto2018off">
<p><span class="smallcaps">Fujimoto, S., Meger, D., and Precup, D.</span> 2018. Off-policy deep reinforcement learning without exploration. <em>arXiv preprint arXiv:1812.02900</em>.</p>
</div>
<div id="ref-gottesman2020interpretable">
<p><span class="smallcaps">Gottesman, O., Futoma, J., Liu, Y., et al.</span> 2020. Interpretable off-policy evaluation in reinforcement learning by highlighting influential transitions. <em>arXiv preprint arXiv:2002.03478</em>.</p>
</div>
<div id="ref-gulcehre2020rl">
<p><span class="smallcaps">Gulcehre, C., Wang, Z., Novikov, A., et al.</span> 2020. RL unplugged: Benchmarks for offline reinforcement learning. <em>arXiv preprint arXiv:2006.13888</em>.</p>
</div>
<div id="ref-hoppe2019qgraph">
<p><span class="smallcaps">Hoppe, S. and Toussaint, M.</span> 2019. Qgraph-bounded q-learning: Stabilizing model-free off-policy deep reinforcement learning..</p>
</div>
<div id="ref-jaques2019way">
<p><span class="smallcaps">Jaques, N., Ghandeharioun, A., Shen, J.H., et al.</span> 2019. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. <em>arXiv preprint arXiv:1907.00456</em>.</p>
</div>
<div id="ref-jiang2016doubly">
<p><span class="smallcaps">Jiang, N. and Li, L.</span> 2016. Doubly robust off-policy value evaluation for reinforcement learning. <em>International conference on machine learning</em>, 652–661.</p>
</div>
<div id="ref-karampatziakis2019empirical">
<p><span class="smallcaps">Karampatziakis, N., Langford, J., and Mineiro, P.</span> 2019. Empirical likelihood for contextual bandits. <em>arXiv preprint arXiv:1906.03323</em>.</p>
</div>
<div id="ref-kidambi2020morel">
<p><span class="smallcaps">Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.</span> 2020. MOReL: Model-based offline reinforcement learning. <em>arXiv preprint arXiv:2005.05951</em>.</p>
</div>
<div id="ref-kumar_blog">
<p><span class="smallcaps">Kumar, A.</span> 2019. Data-driven deep reinforcement learning..</p>
</div> 
<div id=ref-iup2021>
<p><span class="smallcaps">Kumar*, A., Agarwal*, R., Ghosh, D., & Levine, S.</span> 2021. Implicit under-parameterization inhibits data-efficient deep reinforcement learning. <em>ICLR</em>.</p>
</div>
<div id="ref-kumar2019stabilizing">
<p><span class="smallcaps">Kumar, A., Fu, J., Tucker, G., and Levine, S.</span> 2019. Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction. <em>NeurIPS</em>.</p>
</div>
<div id="ref-kumar2020learning">
<p><span class="smallcaps">Kumar, A., Gupta, S. and Malik, J.</span> 2020. Learning Navigation Subroutines from Egocentric Videos. <em>Conference on Robot Learning</em>.</p>
</div> 
<div id="ref-kumar2020conservative">
<p><span class="smallcaps">Kumar, A., Zhou, A., Tucker, G., and Levine, S.</span> 2020. Conservative q-learning for offline reinforcement learning. <em>arXiv preprint arXiv:2006.04779</em>.</p>
</div>
<div id="ref-lagoudakis2003least">
  <p><span class="smallcaps">Lagoudakis, M. G, Parr, R.</span> 2003. Least-squares policy iteration. <em>JMLR</em>.</p>
</div>
<div id="ref-lange2012batch">
<p><span class="smallcaps">Lange, S., Gabel, T., and Riedmiller, M.</span> 2012. Batch reinforcement learning. <em>Reinforcement learning</em>.</p>
</div>
<div id="ref-langford_talk">
<p><span class="smallcaps">Langford, J.</span> 2019. A real-world reinforcement learning revolution..</p>
</div>
<div id="ref-laroche2019safe">
<p><span class="smallcaps">Laroche, R., Trichelair, P., and Des Combes, R.T.</span> 2019. Safe policy improvement with baseline bootstrapping. <em>International conference on machine learning</em>, 3652–3661.</p>
</div>
<div id="ref-levine2020offline">
<p><span class="smallcaps">Levine, S., Kumar, A., Tucker, G., and Fu, J.</span> 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. <em>arXiv preprint arXiv:2005.01643</em>.</p>
</div>
<div id="ref-liu2019understanding">
<p><span class="smallcaps">Liu, Y., Bacon, P.-L., and Brunskill, E.</span> 2019a. Understanding the curse of horizon in off-policy evaluation via conditional importance sampling. <em>arXiv preprint arXiv:1910.06508</em>.</p>
</div>
<div id="ref-liu2019off">
<p><span class="smallcaps">Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E.</span> 2019b. Off-policy policy gradient with state distribution correction. <em>arXiv preprint arXiv:1904.08473</em>.</p>
</div>
<div id="ref-matsushima2020deployment">
<p><span class="smallcaps">Matsushima, T., Furuta, H., Matsuo, Y., Nachum, O., and Gu, S.</span> 2020. Deployment-efficient reinforcement learning via model-based offline optimization. <em>arXiv preprint arXiv:2006.03647</em>.</p>
</div>
<div id="ref-nachum2019dualdice">
<p><span class="smallcaps">Nachum, O., Chow, Y., Dai, B., and Li, L.</span> 2019a. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. <em>Advances in neural information processing systems</em>, 2318–2328.</p>
</div>
<div id="ref-nachum2019algaedice">
<p><span class="smallcaps">Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D.</span> 2019b. AlgaeDICE: Policy gradient from arbitrary experience. <em>arXiv preprint arXiv:1912.02074</em>.</p>
</div>
<div id="ref-nair2020accelerating">
<p><span class="smallcaps">Nair, A., Dalal, M., Gupta, A., and Levine, S.</span> 2020. Accelerating online reinforcement learning with offline datasets. <em>arXiv preprint arXiv:2006.09359</em>.</p>
</div>
<div id="ref-namkoong2020off">
<p><span class="smallcaps">Namkoong, H., Keramati, R., Yadlowsky, S., and Brunskill, E.</span> 2020. Off-policy policy evaluation for sequential decisions under unobserved confounding. <em>arXiv preprint arXiv:2003.05623</em>.</p>
</div>
<div id="ref-peng2019advantage">
<p><span class="smallcaps">Peng, X.B., Kumar, A., Zhang, G., and Levine, S.</span> 2019. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. <em>arXiv preprint arXiv:1910.00177</em>.</p>
</div>
<div id="ref-peshkin2002learning">
<p><span class="smallcaps">Peshkin, L., Shelton, C. R.</span> 2002. Learning from scarce experience. <em>arXiv preprint cs/0204043.</em>.</p>
</div>
<div id="ref-prasad2020defining">
<p><span class="smallcaps">Prasad, N., Engelhardt, B., and Doshi-Velez, F.</span> 2020. Defining admissible rewards for high-confidence policy evaluation in batch reinforcement learning. <em>Proceedings of the acm conference on health, inference, and learning</em>, 1–9.</p>
</div>
<div id="ref-precup2000eligibility">
<p><span class="smallcaps">Precup, D.</span> 2000. Eligibility traces for off-policy policy evaluation. <em>Computer Science Department Faculty Publication Series</em>, 80.</p>
</div>
<div id="ref-precup2001off">
<p><span class="smallcaps">Precup, D., Sutton, R.S., and Dasgupta, S.</span> 2001. Off-policy temporal-difference learning with function approximation. <em>ICML</em>, 417–424.</p>
</div>
<div id="ref-shortreed2011informing">
<p><span class="smallcaps">Shortreed, S.M., Laber, E., Lizotte, D.J., Stroup, T.S., Pineau, J., and Murphy, S.A.</span> 2011. Informing sequential clinical decision-making through reinforcement learning: An empirical study. <em>Machine learning</em>.</p>
</div>
<div id="ref-siegel2020keep">
<p><span class="smallcaps">Siegel, N., Springenberg, J.T., Berkenkamp, F., et al.</span> 2020. Keep doing what worked: Behavior modelling priors for offline reinforcement learning. <em>ICLR</em>.</p>
</div>
<div id="ref-Sohn2020BRPOBR">
<p><span class="smallcaps">Sohn, S., Chow, Y., Ooi, J., et al.</span> 2020. BRPO: Batch residual policy optimization. <em>arXiv:2002.05522</em>.</p>
</div>
<div id="ref-sussexstitched">
<p><span class="smallcaps">Sussex, S., Gottesman, O., Liu, Y., Murphy, S., Brunskill, E., and Doshi-Velez, F.</span> Stitched trajectories for off-policy learning..</p>
</div>
<div id="ref-sutton2009fastgtd">
<p><span class="smallcaps">Sutton, R.S., Maei, H.R., Precup, D., et al.</span> 2009. Fast gradient-descent methods for temporal-difference learning with linear function approximation. <em>Proceedings of the 26th annual international conference on machine learning</em>, 993–1000.</p>
</div>
<div id="ref-sutton1991dyna">
<p><span class="smallcaps">Sutton, R.S.</span> 1991. Dyna, an integrated architecture for learning, planning, and reacting. <em>ACM Sigart Bulletin,</em>, 160-163.</p>
</div>
<div id="ref-tang2021model">
<p><span class="smallcaps">Tang, S. and Wiens, J. </span> 2021. Model Selection for Offline Reinforcement Learning: Practical Considerations for Healthcare Settings. <em>Machine Learning for Healthcare Conference (MLHC)</em>.</p>
</div>
<div id="ref-thomas2015high">
<p><span class="smallcaps">Thomas, P. S., Theocharous, G., Ghavamzadeh, M. </span> 2015. High-confidence off-policy evaluation. <em>AAAI</em>.</p>
</div>
<div id="ref-wang2018">
  <p><span class="smallcaps"> Wang Q, Xiong J, Han L, Liu H, Zhang T. </span> 2018. Exponentially weighted imitation learning for batched historical data. <em> NeurIPS</em>.</p>
</div>
<div id="ref-wang2020critic">
<p><span class="smallcaps">Wang, Z., Novikov, A., Żołna, K., et al.</span> 2020. Critic Regularized Regression. <em>arXiv e-prints</em>, arXiv:2006.15134.</p>
</div>
<div id="ref-wu2019behavior">
<p><span class="smallcaps">Wu, Y., Tucker, G., and Nachum, O.</span> 2019. Behavior regularized offline reinforcement learning. <em>arXiv preprint arXiv:1911.11361</em>.</p>
</div>
<div id="ref-Xie2020QAS">
<p><span class="smallcaps">Xie, T. and Jiang, N.</span> 2020. Q* approximation schemes for batch reinforcement learning: A theoretical comparison. <em>ArXiv</em> <em>abs/2003.03924</em>.</p>
</div>
<div id="ref-yu2020mopo">
<p><span class="smallcaps">Yu, T., Thomas, G., Yu, L., et al.</span> 2020. MOPO: Model-based offline policy optimization. <em>arXiv preprint arXiv:2005.13239</em>.</p>
</div>
</div> -->
